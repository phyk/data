---
title: "Programming Data Science Report"
author: "Philipp Peter, Laurenz Harnischmacher, Nico Lindert"
date: "24 Mai 2019"
output: 
  bookdown::html_document2:
    toc: yes 
    toc_float: true
---
```{r importLibs, include=FALSE}
# load libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringi)
library(hms)
library(knitr)
```


```{r}
#set knitr root
rootDir <- knitr::opts_knit$get("root.dir")
origin_clickstream <- paste(rootDir, "clickstream", sep="/")
origin_experiment <- paste(rootDir,"experiment", sep="/")
origin_orders <- paste(rootDir,"orders", sep="/")

des_root <- paste(rootDir, "00_raw_data", sep="/")
des_under <- paste(rootDir, "01_data_understanding", sep="/")

# move datasets
# clickstream1
unzip(paste(origin_clickstream,"clickstream_data.zip",sep="/"),files= "clickstream_data.csv",list = FALSE, exdir = des_root)
# clickstream2
file.copy(paste(origin_clickstream,"clickstream_data_part_2.csv",sep="/"),des_root)
# experiment 
file.copy(paste(origin_experiment,"experimental_results.csv", sep = "/"),des_root)
#order
file.copy(paste(origin_orders,"order_data.csv",sep = "/"),des_root)

#move .txts
#clickstream_columns.txt
file.copy(paste(origin_clickstream,"clickstream_columns.txt", sep = "/"),des_under)

#order_columns.txt
file.copy(paste(origin_orders,"order_columns.txt",sep = "/"),des_under)


```

require("knitr")

# Set root directory
#knitr::opts_knit$set(root.dir = "C:/Github/data")
knitr::opts_knit$set(root.dir = "C:\\Users\\Philipp\\Documents\\Meine Dokumente\\StudiumDocs\\Master\\2019 SS\\Programming Data Science\\data")

#lh_root
knitr::opts_knit$set(root.dir = "C:\\Users\\Laure\\Documents\\git_Repositories\\phyk_data")

```




```{r warning=FALSE}
# move dataset to raw_data
rootDir <- knitr::opts_knit$get("root.dir")
origin_clickstream <- paste(rootDir, "clickstream", sep="/")
origin_experiment <- paste(rootDir,"experiment", sep="/")
origin_orders <- paste(rootDir,"orders", sep="/")

des_root <- paste(rootDir, "00_raw_data", sep="/")
des_under <- paste(rootDir, "01_data_understanding", sep="/")

# move datasets
# clickstream1
unzip(paste(origin_clickstream,"clickstream_data.zip",sep="/"),files= "clickstream_data.csv",list = FALSE, exdir = des_root)
# clickstream2
file.copy(paste(origin_clickstream,"clickstream_data_part_2.csv",sep="/"),des_root)
# experiment 
file.copy(paste(origin_experiment,"experimental_results.csv", sep = "/"),des_root)
#order
file.copy(paste(origin_orders,"order_data.csv",sep = "/"),des_root)

#move .txts
#clickstream_columns.txt
file.copy(paste(origin_clickstream,"clickstream_columns.txt", sep = "/"),des_under)

#order_columns.txt
file.copy(paste(origin_orders,"order_columns.txt",sep = "/"),des_under)


```

```{r furtherPrep, include=FALSE}
# define data paths
require("knitr")


rootDir <- knitr::opts_knit$get("root.dir")
headersPath <- paste(rootDir, "01_data_understanding/order_columns.txt", sep="/")
headersPath2 <- paste(rootDir, "01_data_understanding/clickstream_columns.txt", sep="/")
dataPath <- paste(rootDir, "00_raw_data/order_data.csv", sep="/")
dataPath2 <- paste(rootDir, "00_raw_data/clickstream_data.csv", sep="/")
dataPath3 <- paste(rootDir, "00_raw_data/clickstream_data_part_2.csv", sep="/")

knitr::opts_chunk$set(echo = TRUE)
```


# Introduction

This course simulated a data science project. Within this course, a webshop's dataset from 2000 will be imported, cleaned, analyzed and evaluated. <br>
The process is separated into the following steps:<br>

* [Data manipulation]
* [Visualization] using a grammar of graphics
* [Summary tables]
* [Comparison of groups] (treatment effect)
* [Predictions] (bias, variance, complexity factor)


## Motivation

In times of big data and the importance of evaluating company's data to stay competible on a highly digitized market, data science provides powerful tools to handle these challenges.<br>

< more blabla >


## Data Import

At first, the csv-files containting data will be imported.

```{r dataImport, echo=FALSE, warning=FALSE, message=FALSE}
# prepare column name list
headersFile = file(headersPath, "r")
headersFile2 = file(headersPath2, "r")
#headerNames <- list()
#http://r.789695.n4.nabble.com/How-to-read-plain-text-documents-into-a-vector-td901794.html
obj_list <- readLines(headersFile)
obj_list2 <- readLines(headersFile2)

#To convert to a vector, do the following:
result <- stri_extract_first(obj_list, regex="[A-z ,]+")
result2 <- stri_extract_first(obj_list2, regex="[A-z ,]+")
dtype <- stri_extract_last(obj_list, regex="[A-z ,]+")
result <- gsub(" ", "_", result)
result2 <- gsub(" ", "_", result2)

order_df <- read_csv(dataPath, col_names = result, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
# NL: Schafft Laptop nicht 
click_df <- read_csv(dataPath2, col_names = result2, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
click_df2 <- read_csv(dataPath3, col_names = result2, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
click_df <- rbind(click_df,click_df2)

# prepare column name list
headersFile = file(headersPath, "r")
headerNames <- list()
#http://r.789695.n4.nabble.com/How-to-read-plain-text-documents-into-a-vector-td901794.html
obj_list <- readLines(headersFile)

#To convert to a vector, do the following:
result <- stri_extract_first(obj_list, regex="[A-z ,]+")
dtype <- stri_extract_last(obj_list, regex="[A-z ,]+")
result <- gsub(" ", "_", result)

dataframe <- read_csv(dataPath, col_names = result, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
dataframe2 <- read_csv(dataPath2, col_names = result, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)


click_df <- rbind(dataframe,dataframe2)
# Parse Order Time
order_df$Order_Time <- parse_time(order_df$"Order_Line_Date_Time","%H\\:%M\\:%S")

# Parse Click Time


click_df$Request_Date_Time <- paste(click_df$Request_Date,click_df2$Request_Date_Time)

click_df$Request_Date_Time <- parse_datetime(click_df$Request_Date_Time,format="%Y-%m-%d %H\\:%M\\:%S")

```


# Analysis

## Data Manipulation {#dataMan}

Due to the huge size of the datasets, we will

1. create a new set `relevant_df` with the 16 most relevant columns
1. reduce the original set `order_df` by deleting all twin columns

Besides, all semicolons within the cells were simply erased.

```{r dataCleanorder, echo=FALSE}
# Clean order_df
# Selected Column Indices
sel <- c(115,116,120,189,233,121,39,101,107,130,135,58,25,30,32,35)

relevant_df <- select(order_df,sel)

# Vergleiche Spalten, ob Dopplungen vorkommen
dataframe <- order_df

drop <- vector()
k <- 1
for (i in 1:ncol(dataframe)) {
  col_i = colnames(dataframe)[i]
  for (j in i+1:ncol(dataframe)){
    col_j = colnames(dataframe)[j]
    if(identical(dataframe[[col_i]],dataframe[[col_j]])) {
      # print(paste(col_i,col_j,sep=" = "))
      drop[k] <- i
      k <- k+1
    }
  }
}
# print(drop)
# Loesche doppelte Spalten (immer die 1.)
order_df <- subset(dataframe, select = -drop)

# Get rid of semicolons
order_df$Estimated_Income_Code <- gsub(";", "", order_df$Estimated_Income_Code)
relevant_df$Estimated_Income_Code <- gsub(";", "", relevant_df$Estimated_Income_Code)
```



```{r dataCleanclick, echo=FALSE}
# Clean click_df



```



## Visualisation

On a first look, the data provides the following information.

```{r firstLook, echo = FALSE, warning = FALSE}





```
Thereby, date and time are less useful, as the time-format is transformed into double values that provide no information on a first look.




## Summary tables

### Session Statistics


```{r Session Statistics, echo=TRUE, warning=TRUE}
#preconditions Session 

Session_data <- click_df %>%
  select(Session_ID,Request_Sequence,Request_Processing_Time,Request_Query_String,Request_Referrer,Request_Date,Request_Date_Time,
         Request_Assortment_ID,Request_Subassortment_ID,Request_Template,REQUEST_DAY_OF_WEEK,
         Product_ID)%>%
  group_by(Session_ID)%>%
  summarise(Request_count = max(Request_Sequence),
            Session_duration_mins = difftime(max(Request_Date_Time),min(Request_Date_Time),units = c("mins")),
            Visited_assortments = length(unique(Request_Assortment_ID)),
            Referrer=last(Request_Referrer),
            Session_day=first(REQUEST_DAY_OF_WEEK),
            Visited_products =length(unique(Product_ID))
            )
Session_data$Session_duration_mins <- as.numeric(Session_data$Session_duration_mins)

df <- data.frame(W=Session_data$Request_count,X=Session_data$Session_duration_mins, Y=Session_data$Visited_assortments, Z=Session_data$Visited_products) # fake data
summary_Session <- df %>% # start with data frame, and then ...
  summarize_at(
    .vars = c("W","X", "Y", "Z"), # select variables that should be in the table
    .funs = funs(min, max, mean, median, sd) # which summary statistics do you want?
  ) %>% 
  gather(key, value) %>% # transpose everything, and then ...
  separate(col=key, sep="_", c("Session_statistics", "stat")) %>% # make two columns out of var name
  spread(key=stat, value=value) # reapeated obs into multiple columns

summary_Session$Session_statistics <- c("Request_count","Session_duration_mins","Visited_assortments","Visited_products")
kable(summary_Session)  


summary_Session





```
From the `click_df` dataset we extractet the `Session_data`dataset.
This dataset contains `r nrow(Session_data)`  session observations.




## Comparison of groups

```{r compare, echo=FALSE}

```


## Predictions

```{r}
library(rpart)
library(caTools)
library(rpart.plot)
require(tree)
library(party)
pred <- click_df
pred <- pred[rowSums(!is.na(pred)) > 0,colSums(!is.na(pred)) > 0]
```




## Market Basket Analysis
```{r prediction, echo=FALSE}
pred <- order_df
pred <- pred %>%
  subset(select=c(Order_ID, Product_Family_ID, Product_Level__2))
```
```{r}
library(plyr)
library(arules)
library(arulesViz)
transactionData <- ddply(pred,c("Order_ID"),
                       function(df1)paste(df1$Product_Family_ID,
                       collapse = ","))
write.csv(transactionData,"market_basket_transactions.csv", quote = FALSE, row.names = FALSE)
tr <- read.transactions('market_basket_transactions.csv', format = 'basket', sep=',')
summary(tr)
if (!require("RColorBrewer")) {
  # install color package of R
install.packages("RColorBrewer")
#include library RColorBrewer
library(RColorBrewer)
}
itemFrequencyPlot(tr,topN=20,type="relative",col=brewer.pal(8,'Pastel2'), main="Absolute Item Frequency Plot")
association.rules <- apriori(tr, parameter = list(supp=0.001, conf=0.95,maxlen=10))
inspect(association.rules)
plot(association.rules, method = "graph",  engine = "htmlwidget")
```

# Conclusion
