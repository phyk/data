---
title: "Programming Data Science Report"
author: "Group number 5: 7308029 Philipp Peter, 7357790 Laurenz Harnischmacher, 7309584 Nico Lindert"
date: "24 Mai 2019"
output: 
  bookdown::html_document2:
    toc: yes 
    toc_float: true
---
```{r importLibs, include=FALSE}
# load libraries
library(tidyverse)
library(ggplot2)
library(dplyr)
library(stringi)
library(stringr)
library(hms)
library(knitr)
library(naniar)
```


```{r setknitr , include=FALSE}
require("knitr")
knitr::opts_chunk$set(error = TRUE)
# Set root directory
knitr::opts_knit$set(root.dir = "C:/Github/data")
#Philipp root
#knitr::opts_knit$set(root.dir = "C:\\Users\\Philipp\\Documents\\Meine Dokumente\\StudiumDocs\\Master\\2019 SS\\Programming Data Science\\data")
#knitr::opts_knit$set(root.dir = "C:\\Users\\Philipp\\Documents\\StudDocs\\Master\\SS2019\\Programming Data Science\\data")
#lh_root
#knitr::opts_knit$set(root.dir = "C:\\Users\\Laure\\Documents\\git_Repositories\\phyk_data")

```


```{r movedatasets, echo=FALSE, warning=FALSE}
# move datasets to raw_data
rootDir <- knitr::opts_knit$get("root.dir")
origin_clickstream <- paste(rootDir, "clickstream", sep="/")
origin_experiment <- paste(rootDir,"experiment", sep="/")
origin_orders <- paste(rootDir,"orders", sep="/")

des_root <- paste(rootDir, "00_raw_data", sep="/")
des_under <- paste(rootDir, "01_data_understanding", sep="/")

# -------- IF THIS IS POSSIBLE, ALL IMPORT AND CLEANSING STEPS CAN BE SKIPPED --------#
didNotImport <- FALSE
tryCatch(load(rdaPathClick),
         error=function(e){
           didNotImport <<- TRUE
         })
tryCatch(load(rdaPathOrder),
         error=function(e){
           didNotImport <<- TRUE
         })

if(didNotImport){ # only necessary, if rdas not available
  # move datasets
  # clickstream1
  unzip(paste(origin_clickstream,"clickstream_data.zip",sep="/"),files="clickstream_data.csv",list = FALSE, exdir = des_root)
  unzip(paste(origin_clickstream,"clickstream_data.zip",sep="/"),files= "clickstream_data.csv",list = FALSE, exdir = des_root)
  # clickstream2
  file.copy(paste(origin_clickstream,"clickstream_data_part_2.csv",sep="/"),des_root)
  # experiment 
  file.copy(paste(origin_experiment,"experimental_results.csv", sep = "/"),des_root)
  #order
  file.copy(paste(origin_orders,"order_data.csv",sep = "/"),des_root)
  
  #move .txts
  #clickstream_columns.txt
  file.copy(paste(origin_clickstream,"clickstream_columns.txt", sep = "/"),des_under)
  
  #order_columns.txt
  file.copy(paste(origin_orders,"order_columns.txt",sep = "/"),des_under)
}
```

```{r furtherPrep, echo=FALSE}
if (didNotImport){
  # define data paths
  require("knitr")
  
  
  rootDir <- knitr::opts_knit$get("root.dir")
  headersPath <- paste(rootDir, "01_data_understanding/order_columns.txt", sep="/")
  headersPath2 <- paste(rootDir, "01_data_understanding/clickstream_columns.txt", sep="/")
  rdaPathOrder <- paste(rootDir, "00_raw_data/order.rda", sep="/")
  rdaPathClick <- paste(rootDir, "00_raw_data/click.rda", sep="/")
  
  # dataPath <- paste(rootDir, "02_data_preparation/order_data_clean.csv", sep="/")
  # dataPath2 <- paste(rootDir, "02_data_preparation/click_data_clean.csv", sep="/")
  
  dataPath <- paste(rootDir, "00_raw_data/order_data.csv", sep="/")
  dataPath2 <- paste(rootDir, "00_raw_data/clickstream_data.csv", sep="/")
  dataPath3 <- paste(rootDir, "00_raw_data/clickstream_data_part_2.csv", sep="/")
  
  knitr::opts_chunk$set(echo = TRUE)
}
```


# Introduction

This course simulated a data science project. Within this course, a webshop's dataset from 2000 will be imported, cleaned, analysed and evaluated. <br>
Along this report, several customer e-mails will be included and guide through the document. Generally, the process is separated into the following steps:<br>

* [Data manipulation]
* [Visualisation] using a grammar of graphics
* [Summary tables]
* [Comparison of groups] (treatment effect)
* [Predictions] (bias, variance, complexity factor)


## Scenario

In times of big data and the importance of evaluating company's data to stay competible on a highly digitised market, data science provides powerful tools to handle these challenges.<br>
Therefore, a fictive company called "ebuy" wanted us to analyse the data that their webshop produced. Two datasets about order details and clicksstreams were available and the first step was to produce "interesting numbers and images that are easy to understand". In a later request, a second clickstream file was uploaded and should be considered as well. <br>
The following chapters [Data Import], [Data Manipulation] and [Visualisation] will deal with answering these requests.


## Data Import

Before any visualisation or even analysis can take place, the data has to be imported, cleaned and prepared. This included

1. Extracting the zip-packages
1. Reading csv-files and transforming them to modifiable data frames
1. Merging the two clickstream files to one data frame

```{r dataImport, include=FALSE}
if(didNotImport)
{
  # prepare column name list
  headersFile = file(headersPath, "r")
  headersFile2 = file(headersPath2, "r")
  #headerNames <- list()
  #http://r.789695.n4.nabble.com/How-to-read-plain-text-documents-into-a-vector-td901794.html
  obj_list <- readLines(headersFile)
  obj_list2 <- readLines(headersFile2)
  
  #To convert to a vector, do the following:
  result <- stri_extract_first(obj_list, regex="[A-z ,]+")
  result2 <- stri_extract_first(obj_list2, regex="[A-z ,]+")
  dtype <- stri_extract_last(obj_list, regex="[A-z ,]+")
  result <- gsub(" ", "_", result)
  result2 <- gsub(" ", "_", result2)
  
  # initially
  # order_df <- read_csv(dataPath, col_names = result, na=c("", "?", "NULL", "NA", "Nan"))
  # click_df <- read_csv(dataPath2, col_names = result2, na=c("", "?", "NULL", "NA", "Nan"))
  
  # order_df <- read_csv(dataPath, na=c("", "?", "NULL", "NA", "Nan"))
  # click_df <- read_csv(dataPath2, na=c("", "?", "NULL", "NA", "Nan"))
  
  order_df <- read_csv(dataPath, col_names = result, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
  # NL: Schafft Laptop nicht 
  click_df <- read_csv(dataPath2, col_names = result2, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
  click_df2 <- read_csv(dataPath3, col_names = result2, na=c("", "?", "NULL", "NA", "Nan"), guess_max = 3400)
  click_df <- rbind(click_df,click_df2)
  
  # Parse Order Time
  order_df$Order_Time <- parse_time(order_df$"Order_Line_Date_Time","%H\\:%M\\:%S")
  
  
  # Parse Click Time
  click_df$Request_Date_Time <- paste(click_df$Request_Date,click_df2$Request_Date_Time)
  click_df$Request_Date_Time <- parse_datetime(click_df$Request_Date_Time,format="%Y-%m-%d %H\\:%M\\:%S")
}
```

## Data Manipulation {#dataMan}

A few things within the datasets had to be fixed.

```{r cleaningFunctions, echo=FALSE}
cleanNas <- function(x){
  df2 <- x %>% replace_with_na_all(condition = ~.x %in% c("?", "NULL"))
  ## dataframe <- str_replace_all(dataframe, ";", ",")
  return (df2)
}
dropEmptyRowsAndCols <- function(x){
  return(x[rowSums(!is.na(x)) > 0,colSums(!is.na(x)) > 0])
}

dropcounter <- 0
dropDuplicateCols <- function(x)
{
  drop <- vector()
  k <- 1
  for (i in 1:ncol(x)) {
    col_i = colnames(x)[i]
    for (j in i+1:ncol(x)){
      col_j = colnames(x)[j]
      if(identical(x[[col_i]],x[[col_j]])) {
        # print(paste(col_i,col_j,sep=" = "))
        drop[k] <- i
        k <- k + 1
        dropcounter <- dropcounter + 1
      }
    }
  }
  temp <- x[, !colnames(x) %in% drop]
  return(temp)
}
deleteSemicolons <- function(x)
{
  return(gsub(";","",x))
}
deleteBackslashs <- function(x)
{
  return(gsub("\\\\","",x,fixed=TRUE))
}
dropColumnsPercNA <- function(x, percentage)
{
  lengthX <- length(x[[1]])
  k <- 0
  drop <- vector()
  for (i in 1:ncol(x))
  {
    col_i <- colnames(x)[i]
    if(((lengthX - sum(is.na(x[col_i]))) / lengthX) < percentage)
    {
      drop[k] <- col_i
      k <- k+1
    }
  }
  temp <- x[, !colnames(x) %in% drop]
  return(temp)
}
```

```{r dataCleanorder, echo=FALSE}
# Clean order_df
# Selected Column Indices
sel <- c("Order_Session_ID","Order_Date","Order_Time","Order_ID","Order_Status","Order_Amount","HowDidYouHearAboutUs","City","US_State","Year_of_Birth","Customer_ID","Estimated_Income_Code","Gender","Order_Credit_Card_Brand","BrandName","Product_Object_ID","Assortment_ID")
sel2 <- c("Session_ID","Request_Sequence","Request_Processing_Time","Request_Query_String","Request_Referrer","Request_Date","Request_Date_Time","Request_Assortment_ID","Request_Subassortment_ID","Request_Template","REQUEST_DAY_OF_WEEK","Product_ID")

or <- select(order_df,sel)
cl <- select(click_df, sel2)

# Get rid of semicolons
or$Estimated_Income_Code <- gsub(";", "", or$Estimated_Income_Code)

#write.csv(or, "order_data_clean.csv")
#write.csv(cl, "click_data_clean.csv")
```


```{r dataCleaningOrder, echo=FALSE}
#if(didNotImport)
#{
  order_df <- order_df %>% 
    cleanNas() %>%
    dropEmptyRowsAndCols() %>%
    dropDuplicateCols()
  dropcount_order <- dropcounter

  order_df$Estimated_Income_Code <- deleteSemicolons(order_df$Estimated_Income_Code)
  
  time_list = c(
    order_df$Order_Date_Time,
    order_df$Promotion_Object_Modification_Date_Time,
    order_df$Last_Retail_Date_Time,
    order_df$Verification_Date_Time,
    order_df$Last_Update_Date_Time,
    order_df$Order_Line_Date_Time,
    order_df$Account_Creation_Date_Time,
    order_df$Product_Creation_Date_Time,
    order_df$Assortment_Creation_Date_Time,
    click_df$Content_Creation_Date_Time,
    click_df$Product_Creation_Date_Time,
    click_df$Content_Modification_Date_Time,
    click_df$Product_Modification_Date_Time,
    click_df$Assortment_Creation_Date_Time,
    click_df$Cookie_First_Visit_Date_Time,
    click_df$Assortment_Modification_Date_Time,
    click_df$Session_First_Request_Date_Time
    )
  
  for (time in time_list) time <- deleteBackslashs(time)
#}
```



```{r dataCleanclick, echo=FALSE}
if(didNotImport)
{
  # Clean click_df
  click_df <- click_df %>% 
    dropEmptyRowsAndCols() %>%
    dropDuplicateCols
  dropcount_click <- dropcounter
  
  click_df$LeadsToBuy <- click_df$Request_Template == "checkout/thankyou\\.jhtml"
}
```

1. Empty cells had several different values, e.g. `?` or `NULL`. All of these were changed to R-modifiable `NA` values.
1. Empty rows/observations and columns/features were dropped.
1. Duplicate features were dropped as well. This affected `r dropcount_order` columns in the order dataset and `r dropcount_click` columns in the clickstream dataset.
1. Column names included spaces that became problematic in later analysis. They were simply replaced by underscores.
1. All semicolons within cells were simply erased.
1. Besides, backslashes within cells were erased as well.

```{r exportDataframes, echo=FALSE}
# ---------- This step enables faster knitting and progress after cleaning is done once. --------- #
if(didNotImport)
{
  save(click_df, rdaPathClick)
  save(order_df, rdaPathOrder)
}
```


## Visualisation

On a first look, the data provides the following information.<br>
Along the whole timeframe of 

```{r grammarofg, echo=FALSE}
#x = as.POSIXct(paste(or$Order_Date, or$Order_Time), format="%Y-%m-%d %H:%M:%S")

ggplot(or) +
  geom_point(mapping = aes(x = Order_Time, y = Order_Amount, color = Gender))

or$Estimated_Income_Code <- factor(or$Estimated_Income_Code, levels = c(
  "Under $15000", "$15000-$19999", "$20000-$29999", "$30000-$39999", "$40000-$49999",
  "$50000-$74999", "$75000-$99999", "$100000-$124999", "$125000 OR MORE"
))

boxplot(Order_Amount~Gender, data = order_df, horizontal = TRUE)
boxplot(Order_Amount~Estimated_Income_Code, data = order_df)

```

```{r firstLook , echo=FALSE}


knitr::knit_exit()
```
Thereby, date and time are less useful, as the time-format is transformed into double values that provide no information on a first look.




# Analysis

## Summary tables

### Clickstream tables

```{r Clickstreamtables , echo=FALSE}
buffer<- click_df %>%
  select(Session_ID,Request_Query_String,Request_Referrer,Request_Assortment_ID,Request_Subassortment_ID,Request_Template,REQUEST_DAY_OF_WEEK,
         Product_ID)%>%
  group_by(Session_ID)

buffer$Request_Referrer<- str_split(buffer$Request_Referrer,"/")

buffer$Request_Referrer <- sapply(buffer$Request_Referrer,grep,pattern="www",value=TRUE,invert=FALSE)

buffer$Request_Referrer<- replace_na(buffer$Request_Referrer)

buffer$Request_Referrer <- as.character(buffer$Request_Referrer)

buffer <- buffer %>%
  select(Request_Referrer,Session_ID) %>%
  group_by(Request_Referrer) %>%
  summarise(Number_of_Referrers = n())

buffer <- filter(buffer,Request_Referrer != "NA")

buffer <-buffer[order(-buffer$Number_of_Referrers),]

Session_top5 <- buffer[1:5,1:2]

buffer<- click_df %>%
  select(Session_ID,Product,Product_ID) %>%
  group_by(Product)%>%
  summarise(Click_on_Product = n())

buffer <- filter(buffer,!is.na(Product))
buffer <-buffer[order(-buffer$Click_on_Product),]

Session_top5[1:5,3:4] <- buffer[1:5,1:2]


buffer <- click_df %>%
  select(Session_ID,REQUEST_HOUR_OF_DAY) %>%
  group_by(REQUEST_HOUR_OF_DAY) %>%
  summarise(Clicks_per_hours = n())

buffer <-buffer[order(-buffer$Clicks_per_hours),]


Session_top5[1:5,5:6] <- buffer[1:5,1:2]


buffer <- click_df %>%
  select(Session_ID,REQUEST_DAY_OF_WEEK) %>%
  group_by(REQUEST_DAY_OF_WEEK) %>%
  summarise(Clicks_per_day = n())

buffer <-buffer[order(-buffer$Clicks_per_day),]

Session_top5[1:5,7:8] <- buffer[1:5,1:2]

names(Session_top5)<- c("Referrer","Number_of_Referres","Product","Clicks_on_Product",
                      "Top_Click_Hours","Clicks_per_Hour","Top_Click_Days","Clicks_per_Day")

Session_top5 <-as.data.frame(t(Session_top5))
names(Session_top5)<- c("Top1","Top2","Top3","Top4","Top5")


kable(Session_top5)
```

In the `click_df` we united the first clickstream dataset and the secound clickstream dataset. Together the `click_df` contains `r nrow(click_df)` click observations. We identifyed the top five from the `click_df` which are show above.



```{r Sessiontable, echo=FALSE, warning=FALSE}

ave_Session_df <- click_df %>%
  select(Session_ID,Request_Sequence,Request_Processing_Time,Request_Query_String,Request_Referrer,Request_Date,Request_Date_Time,
         Request_Assortment_ID,Request_Subassortment_ID,Request_Template,REQUEST_DAY_OF_WEEK,LeadsToBuy,Session_First_Request_Day_of_Week,
         Product_ID,Session_First_Request_Hour_of_Day,Session_User_Agent)%>%
  group_by(Session_ID)%>%
  summarise(Request_count = max(Request_Sequence),
            Session_duration_mins = difftime(max(Request_Date_Time),min(Request_Date_Time),units = c("mins")),
            Visited_assortments = length(unique(Request_Assortment_ID)),
            Referrer=last(Request_Referrer),
            Session_day=first(REQUEST_DAY_OF_WEEK),
            Visited_products =length(unique(Product_ID)),
            LeadsToBuy = sum(LeadsToBuy) > 0,
            Session_day_2 = first(Session_First_Request_Day_of_Week),
            Session_hour = first(Session_First_Request_Hour_of_Day),
            Visited_Products_List = paste(Product_ID, collapse = ",")
            )
  
ave_Session_df$Session_duration_mins <- as.numeric(ave_Session_df$Session_duration_mins)

# inserted Referrer for ML
ave_Session_df$Referrer <- sapply(ave_Session_df$Referrer,str_extract,pattern="[[:alpha:]]+\\\\\\.([[:alnum:]]+)\\\\\\.[[:alnum:]]+")

ave_Session_df$Referrer<- replace_na(ave_Session_df$Referrer, "none")

ave_Session_df$Referrer <- as.factor(ave_Session_df$Referrer)

df <- data.frame(W=ave_Session_df$Request_count,X=ave_Session_df$Session_duration_mins, Y=ave_Session_df$Visited_assortments, Z=ave_Session_df$Visited_products) # fake data
summary_stats <- df %>% # start with data frame, and then ...
  summarize_at(
    .vars = c("W","X", "Y", "Z"), # select variables that should be in the table
    .funs = funs(min, max, mean, median, sd) # which summary statistics do you want?
  ) %>% 
  gather(key, value) %>% # transpose everything, and then ...
  separate(col=key, sep="_", c("summary_stats", "stat")) %>% # make two columns out of var name
  spread(key=stat, value=value) # reapeated obs into multiple columns

summary_stats$summary_stats <- c("Request_count","Session_duration_mins","Visited_assortments","Visited_products")

kable(summary_stats) 
```

From the `click_df` dataset we extractet the `ave_Session_df`dataset.
This dataset contains `r nrow(ave_Session_df)`  session observations.
The tabel above summarizes  the average session. 

### Order tables

```{r Ordertables, echo=FALSE}

buffer_order <- order_df %>%
  select(Order_ID,Order_Line_ID,Order_Line_Quantity,Product_ID,Order_Date_Time,Order_Promotion_Code,
         Order_Discount_Amount,Order_Line_Unit_List_Price,Order_Line_Unit_Sale_Price,Order_Amount,
         Order_Customer_ID,Product,Order_Credit_Card_Payment_Amount,BrandName,Order_Hour_of_Day,Order_Day_of_Week) %>%
  group_by(Order_ID)

buffer <- buffer_order %>%
  select(Order_ID,Product_ID,Product,Order_Line_Quantity) %>%
  group_by(Product_ID) %>%
  summarise(Order_ID = n(),
            Product = last(Product),
            Product_Quantity=sum(Order_Line_Quantity))
buffer <- filter(buffer,!is.na(Product_ID))
buffer <- buffer[order(-buffer$Product_Quantity),]

Order_top5 <- buffer[1:5,1]
Order_top5[1:5,2:2] <- buffer[1:5,4] 

buffer <- filter(buffer,!is.na(Product))
Order_top5[1:5,3:4] <- buffer[1:5,3:4] 

buffer <- buffer_order %>%
  select(Order_Customer_ID,Order_ID,Order_Line_Quantity)%>%
  group_by(Order_Customer_ID)%>%
  summarise(total_orders = n(),Products_Ordered=sum(Order_Line_Quantity))
buffer <- buffer[order(-buffer$total_orders),]
Order_top5[1:5,5:6] <- buffer[1:5,1:2] 

buffer <- buffer[order(-buffer$Products_Ordered),]
Order_top5[1:5,7:7] <- buffer[1:5,1] 
Order_top5[1:5,8:8] <- buffer[1:5,3] 

buffer <- buffer_order %>%
  select(Product_ID,Order_ID,BrandName,Order_Line_Quantity) %>%
  group_by(BrandName) %>%
  summarise(Order_ID = n(),
            Product_Quantity=sum(Order_Line_Quantity))
buffer <- filter(buffer,!is.na(BrandName))
buffer <- buffer[order(-buffer$Product_Quantity),]
Order_top5[1:5,9:9] <- buffer[1:5,1] 
Order_top5[1:5,10:10] <- buffer[1:5,3] 

buffer <- buffer_order %>%
  select(Order_ID,Order_Line_Quantity,Order_Hour_of_Day)%>%
  group_by(Order_Hour_of_Day)%>%
  summarise(Orders = n())
buffer <- buffer[order(-buffer$Orders),]
Order_top5[1:5,11:12] <- buffer[1:5,1:2]

buffer <- buffer_order %>%
  select(Order_ID,Order_Line_Quantity,Order_Day_of_Week)%>%
  group_by(Order_Day_of_Week)%>%
  summarise(Orders = n())
buffer <- buffer[order(-buffer$Orders),]
Order_top5[1:5,13:14] <- buffer[1:5,1:2]

names(Order_top5)<- c("Product_ID","Orders_per_Product_ID","Product_Name","Orders_per_Product_Name",
                      "Customer_ID","Orders_per_Customer","Customer__ID","Purchased_Products_per_Customer",
                      "Brand","Orders_per_Brand","Top_Sale_Hours","Orders_per_Hours",
                      "Top_Sale_Days","Orders_per_Day")

Order_top5 <-as.data.frame(t(Order_top5))
names(Order_top5)<- c("Top1","Top2","Top3","Top4","Top5")


kable(Order_top5)

```


```{r ReturnSummary, echo=FALSE, warning=FALSE}

# create return average table
buffer <- filter(order_df,Order_Line_Quantity<0)
ave_Return <- buffer %>%
  select(Order_ID,Order_Line_ID,Order_Line_Quantity,Product_ID,Order_Date_Time,Order_Promotion_Code,Order_Discount_Amount,Order_Line_Unit_List_Price,Order_Line_Unit_Sale_Price,Order_Amount) %>%
  group_by(Order_ID) %>%
  summarise(Order_Line_count = n(),
            Order_Quantity = sum(Order_Line_Quantity)*-1,
            Purchased_Products = length(unique(Product_ID)),
            Used_Promotions = ifelse(is.na(unique(Order_Promotion_Code)),0,1),
            Used_Promotion = unique(Order_Promotion_Code),
            Discount_Amount = max(Order_Discount_Amount),
            Discount_from_Sale_Price = sum(sum(Order_Line_Unit_List_Price*Order_Line_Quantity)-sum(Order_Line_Unit_Sale_Price*Order_Line_Quantity))*-1,
            Order_Amount = max(Order_Amount))
df <- data.frame(U=ave_Return$Order_Quantity ,V=ave_Return$Purchased_Products,W=ave_Return$Used_Promotions,
                 X=ave_Return$Discount_Amount, Y=ave_Return$Discount_from_Sale_Price, Z=ave_Return$Order_Amount) # fake data
summary_Return <- df %>% # start with data frame, and then ...
  summarize_at(
    .vars = c("U","V","W","X", "Y", "Z"), # select variables that should be in the table
    .funs = funs(min, max, mean, median, sd) # which summary statistics do you want?
  ) %>% 
  gather(key, value) %>% # transpose everything, and then ...
  separate(col=key, sep="_", c("summary_Return", "stat")) %>% # make two columns out of var name
  spread(key=stat, value=value) # reapeated obs into multiple columns

summary_Return$summary_Return <- c("Return_Quantity","Returned_Products","Used_Promotions",
                                   "Discount_Amount","Discount_from_Sale_Price","Return_Amount")

kable(summary_Return) 


```

The table "Summary_Return" is based on `r nrow(ave_Return)` return observations. We extracted these return from the order dataset.


```{r OrderSummary, echo=FALSE, warning=FALSE}

# create order average table
buffer <- filter(order_df,Order_Line_Quantity>0)
ave_Order <- buffer %>%
  select(Order_ID,Order_Line_ID,Order_Line_Quantity,Product_ID,Order_Date_Time,Order_Promotion_Code,Order_Discount_Amount,Order_Line_Unit_List_Price,Order_Line_Unit_Sale_Price,Order_Amount,Order_Credit_Card_Payment_Amount) %>%
  group_by(Order_ID) %>%
  summarise(Order_Line_count = n(),
            Order_Quantity = sum(Order_Line_Quantity),
            Purchased_Products = length(unique(Product_ID)),
            Used_Promotions = ifelse(is.na(unique(Order_Promotion_Code)),0,1),
            Used_Promotion = unique(Order_Promotion_Code),
            Discount_Amount = max(Order_Discount_Amount),
            Discount_from_Sale_Price = sum(sum(Order_Line_Unit_List_Price*Order_Line_Quantity)-sum(Order_Line_Unit_Sale_Price*Order_Line_Quantity)),
            Order_Amount = max(Order_Amount),
            Creditcard_Payment_Amount = max(Order_Credit_Card_Payment_Amount))
df <- data.frame(U=ave_Order$Order_Quantity ,V=ave_Order$Purchased_Products,W=ave_Order$Used_Promotions,
                 X=ave_Order$Discount_Amount, Y=ave_Order$Discount_from_Sale_Price, Z=ave_Order$Order_Amount,
                 ZA=ave_Order$Creditcard_Payment_Amount) 
df$X <- replace(df$X,is.na(df$X),0)
df$ZA <- replace(df$ZA,is.na(df$ZA),0)
summary_Order <- df %>% # start with data frame, and then ...
  summarize_at(
    .vars = c("U","V","W","X", "Y", "Z","ZA"), # select variables that should be in the table
    .funs = funs(min, max, mean, median, sd) # which summary statistics do you want?
  ) %>% 
  gather(key, value) %>% # transpose everything, and then ...
  separate(col=key, sep="_", c("summary_Order", "stat")) %>% # make two columns out of var name
  spread(key=stat, value=value) # reapeated obs into multiple columns

summary_Order$summary_Order <- c("Order_Quantity","Purchased_Products","Used_Promotions",
                                   "Discount_Amount","Discount_from_Sale_Price","Order_Amount",
                                 "Creditcard_Payment_Amount")
kable(summary_Order) 


```

The table "Summary_Order" is based on `r nrow(ave_Order)` order observations. We extracted these orders from the order dataset.The negative minimum in the row b